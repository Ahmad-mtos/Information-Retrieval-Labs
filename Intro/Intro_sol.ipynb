{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95edd0c6-b44e-40d6-9e9c-48f42ddd5207",
   "metadata": {},
   "source": [
    "# 0. Installing course dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ae1d575-f278-450f-aa6f-d99e075a79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt\n",
    "!conda install -c conda-forge ffmpeg -y\n",
    "!python -m spacy download en_trf_distilbertbaseuncased_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7bedc-d559-437e-b106-12c860acfbab",
   "metadata": {},
   "source": [
    "# 1. Touching the Internet\n",
    "\n",
    "Solve the following task. Download [this page](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt), and save it to the file with the name derived from the URL. File with another URL should not be save into the file with this name. E.g. [this file](https://github.com/IUCVLab/information-retrieval/blob/main/datasets/facts.txt).\n",
    "\n",
    "Ref: [requests](https://docs.python-requests.org/en/latest/) library is cool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639fb2d7-d577-4ae6-beb4-2487213024cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os.path \n",
    "from hashlib import sha512\n",
    "import base64\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "r = requests.get(url)\n",
    "hash_filename= \"./\" + sha512(url.encode()).hexdigest() + \".txt\"\n",
    "\n",
    "with open(hash_filename, 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7601aaf-3534-42cd-bb33-665d2d92c79d",
   "metadata": {},
   "source": [
    "# 2. Parsing different formats\n",
    "\n",
    "Most probably, if you meet something in Internet, this is: binary, plain text, XML, or json. XML also splits into xHTML, RSS, Atom, SOAP, XML-RPC, ... . Your task is to learn, how to process different formats.\n",
    "\n",
    "## 2.1. JSON\n",
    "\n",
    "In [the given file](http://sprotasov.ru/data/postnauka.txt) there is valid json. Parse it and print all video URLs, which have `computer science` tag. Use built-in features of `requests`, or just a `json` library ([ref](https://docs.python.org/3/library/json.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628c2c18-b896-40f9-bac9-2fefc5027da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://postnauka.ru/talks/31897', 'http://postnauka.ru/video/24306', 'http://postnauka.ru/faq/46974']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"http://sprotasov.ru/data/postnauka.txt\"\n",
    "resp = requests.get(url=url)\n",
    "data = json.loads(resp.text.encode().decode('utf-8-sig') )\n",
    "urls = []\n",
    "for dict in data:\n",
    "    if 'computer science' in dict['tags']:\n",
    "        urls.append(dict['url'])\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97530a0-46d4-47e3-a7bb-ca479680007d",
   "metadata": {},
   "source": [
    "## 2.2. HTML\n",
    "\n",
    "For a given StackExchange answer extract logins of the contributors (who asked and who answered) with votes. [bs4](https://beautiful-soup-4.readthedocs.io/en/latest/) will help you to do the job.\n",
    "\n",
    "I can recommend to use CSS or XPath selectors. `div` elements with `post-layout` class represent answers. Inside there are `div` with `votecell` class stroring votes number and `div` with class `user-details` storing user info. My personal recommendation is to use `css selectors`, which are [documented here](https://beautiful-soup-4.readthedocs.io/en/latest/#css-selectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67db473-a8f0-414c-adbc-ce0e9e4710bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://math.stackexchange.com/questions/411486/understanding-the-singular-value-decomposition-svd\n",
      "23 Rodrigo de Azevedo\n",
      "17 Ittay Weiss\n",
      "9 None\n",
      "4 Bart Vanderbeke\n",
      "3 Bart Vanderbeke\n",
      "2 hgfei\n",
      "1 littleO\n",
      "1 TheSHETTY-Paradise\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = f\"https://math.stackexchange.com/questions/411486/understanding-the-singular-value-decomposition-svd\"\n",
    "print(url)\n",
    "\n",
    "resp = requests.get(url=url)\n",
    "soup = resp.text\n",
    "soup = BeautifulSoup(soup)\n",
    "mydivs = soup.select(\"div.post-layout\")\n",
    "# print(mydivs[0])\n",
    "votevals = []\n",
    "names = []\n",
    "for div in mydivs:\n",
    "    vote_num = div.select(\"div.js-vote-count\")[0]\n",
    "    votevals.append(int(vote_num.get_text().replace('\\n', \"\").replace('\\r', \"\").replace(\" \", \"\")))\n",
    "    name = div.select(\"div.user-details\")[0]\n",
    "    try:\n",
    "        name = name.find_all('a')[0].getText()\n",
    "    except IndexError:\n",
    "        name = \"None\"\n",
    "    names.append(name)#.split(\"\\n\"))#[1])\n",
    "    \n",
    "for vote, name in zip(votevals, names):\n",
    "    if vote:\n",
    "        print(vote, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708766b-0db3-4062-87a1-9ba96c60440b",
   "metadata": {},
   "source": [
    "# 2.3. RSS feed\n",
    "\n",
    "A lot of information is already organized in typed XML documents. Podcasts are just RSS feed. Parse [the feed of this podcast](http://sprotasov.ru/podcast/rss.xml) and print out the time span between the first and the last episodes. Use [`feedparser` for this](https://waylonwalker.com/parsing-rss-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2090f810-d706-4bb2-8c85-d485a48432a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1801 days, 10:30:00\n",
      "podcast took 4 years, 11 months, 11 days\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "from datetime import datetime, timedelta\n",
    "rss = 'http://sprotasov.ru/podcast/rss.xml'\n",
    "feedparser.parse(rss) \n",
    "entries = feedparser.parse(rss)['entries']\n",
    "\n",
    "#if to calculate duration\n",
    "# feeds = [entry['itunes_duration'] for entry in entries]\n",
    "# times = []\n",
    "# for datetime_str in feeds:\n",
    "#     datetime_object = (datetime.strptime(datetime_str, '%H:%M:%S')-datetime(1900, 1, 1)).total_seconds()\n",
    "#     times.append(datetime_object)\n",
    "# print(timedelta(seconds=sum(times)))\n",
    "\n",
    "#if to calculate anual period\n",
    "feeds = [entry['published_parsed'] for entry in entries]\n",
    "s_year, s_mon, s_mday, s_hour, s_min, s_sec, _, _, _ = feeds[-1]\n",
    "e_year, e_mon, e_mday, e_hour, e_min, e_sec, _, _, _ = feeds[0]\n",
    "times = []\n",
    "s_datetime_object = datetime(*[s_year, s_mon, s_mday, s_hour, s_min, s_sec])\n",
    "e_datetime_object = datetime(*[e_year, e_mon, e_mday, e_hour, e_min, e_sec])\n",
    "diff = e_datetime_object - s_datetime_object\n",
    "print(diff)\n",
    "\n",
    "# the following compuatation is not very precise!!!! \n",
    "# (as it does not account particular days and years)\n",
    "# https://stackoverflow.com/a/4040338\n",
    "days = diff.days\n",
    "years = days//365\n",
    "months = (days-365*years)//30\n",
    "days = days - years*365 - months*30\n",
    "print(\"podcast took {} years, {} months, {} days\".format(years, months, days))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a63b1-c106-4a13-8e55-9240e9c8418f",
   "metadata": {},
   "source": [
    "# 3. Solving simple information retrieval task\n",
    "\n",
    "According to the name, `information retrieval` is the discipline, which helps retrieves information (from unstructured sources). Thus, we will retrieve some information from [this news article](https://www.bbc.com/news/world-us-canada-59944889). Your task is to write a code, which will answer the question: **How many people die every day in the US waiting for a transplant?** Write flexible enough code. Test yourself by changing the link to [this one](https://www.americantransplantfoundation.org/about-transplant/facts-and-myths/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7660c706-371b-4050-aede-e4b3e4014ce1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Currently 17 people die every day in the US waiting for a transplant, with more than 100,000 reportedly on the waiting list\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://www.americantransplantfoundation.org/about-transplant/facts-and-myths/'\n",
    "url = 'https://www.bbc.com/news/world-us-canada-59944889'\n",
    "question = 'How many people die every day in the US waiting for a transplant?'\n",
    "\n",
    "resp = requests.get(url=url)\n",
    "soup = resp.text\n",
    "soup = BeautifulSoup(soup)\n",
    "mydivs = soup.find_all(\"body\")[0].text.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "text = \"\"\n",
    "for char in mydivs:\n",
    "    text+=char\n",
    "text = text.split('.')\n",
    "bow = question.split()\n",
    "scores = []\n",
    "for sentence in text:\n",
    "    score = 0\n",
    "    for word in bow:\n",
    "        if word in sentence: score+=1\n",
    "    scores.append(score)\n",
    "index = scores.index(max(scores))\n",
    "print(text[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
