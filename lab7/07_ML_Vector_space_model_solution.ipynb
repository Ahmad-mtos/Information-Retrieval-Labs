{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGg1iXheYejS"
      },
      "source": [
        "# Vector space with ML\n",
        "\n",
        "This lab will be devoted to the use of ML model for the needs of information retrieval and text classification.  \n",
        "\n",
        "**Searching in the curious facts database**\n",
        "\n",
        "The facts dataset is given [here](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt), take a look. We want you to retrieve facts **relevant to the query** (whatever it means), for example, you type \"good mood\", and get to know that Cherophobia is the fear of fun. For this, the idea is to utilize document vectors. However, instead of forming vectors with tf-idf and reducing dimensions, this time we want to obtain fixed-size vectors for documents using ML model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHTMD32SYejZ"
      },
      "source": [
        "## 1. Use neural networks to embed sentences\n",
        "\n",
        "Make use of any, starting from doc2vec up to Transformers, etc. Provide all code, dependencies, installation requirements.\n",
        "\n",
        "\n",
        "- [UCE in spacy 2](https://spacy.io/universe/project/spacy-universal-sentence-encoder) (`!pip install spacy-universal-sentence-encoder`)\n",
        "- [Sentence BERT in spacy 2](https://spacy.io/universe/project/spacy-sentence-bert) (`!pip install spacy-sentence-bert`)\n",
        "- [Pretrained ðŸ¤— Transformers](https://huggingface.co/transformers/pretrained_models.html)\n",
        "- [Spacy 3 transformers](https://spacy.io/usage/embeddings-transformers#transformers-installation)\n",
        "- [doc2vec pretrained](https://github.com/jhlau/doc2vec)\n",
        "- [Some more sentence transformers](https://www.sbert.net/docs/quickstart.html)\n",
        "- [Even fasttext can do a sentence embedding](https://fasttext.cc/docs/en/python-module.html#model-object)\n",
        "\n",
        "Here should be dependency installation, download instructions and so on. With outputs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "embed_hub = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
      ],
      "metadata": {
        "id": "RmLIGcf_HABm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V34hgE5DYeje"
      },
      "source": [
        "And then use the library to download (and load) the model:\n",
        "\n",
        "NB: model downloading may take time (depending on the model hosting). If you think it may take a long time, ask your TA for assistance with binaries."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = embed_hub([\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"I am a sentence for which I would like to get its embedding\",\n",
        "    \"hasdf\"])\n",
        "\n",
        "print(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JV8r7cnLiXcU",
        "outputId": "57be9704-09a1-402a-d645-8279486b651a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.01305108  0.02235124 -0.03263273 ... -0.00565096 -0.04793027\n",
            "  -0.11492759]\n",
            " [ 0.0583339  -0.08185007  0.06890936 ... -0.00923876 -0.08695352\n",
            "  -0.01415743]\n",
            " [-0.02259955  0.03317683  0.04738321 ...  0.01560432 -0.01312117\n",
            "  -0.05859604]], shape=(3, 512), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_CKj1PkYejf"
      },
      "source": [
        "## 2. Write a function that prepares embedding of arbitrary queries\n",
        "\n",
        "Write a function, which returns a fixed-sized vector of embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AjaeAj8Yejg"
      },
      "outputs": [],
      "source": [
        "# import fasttext\n",
        "import numpy as np\n",
        "# ft = fasttext.load_model('cc.en.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnq4Z3mqYejh"
      },
      "outputs": [],
      "source": [
        "def embed(text):\n",
        "    return embed_hub([text])[0].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoIPL_t7Yeji"
      },
      "source": [
        "Here we check that embeddings are of the same size and type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhXc9hEIYejj"
      },
      "outputs": [],
      "source": [
        "assert embed(\n",
        "            \"Some random text\"\n",
        "        ).shape == \\\n",
        "        embed(\n",
        "            \"Folks, here's a story about Minnie the Moocher. \"\n",
        "            \"She was a lowdown hoochie coocher. \"\n",
        "            \"She was the roughest, toughest frail, \"\n",
        "            \"but Minnie had a heart as big as a whale\"\n",
        "        ).shape, \"Shape should match\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rHj6iLhYejj"
      },
      "source": [
        "NB: here we check DISTANCE, not similarity. This similar texts should produce results close to 0."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "cosine(\n",
        "            embed(\"some text for testing\"), \n",
        "            embed(\"some text for testing\")\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x-BASMhJLyC",
        "outputId": "4e97a13a-e78d-4b17-dcf2-3f76a6cb2437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.960464477539063e-08"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaTQN-hiYejj"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "assert abs(cosine(\n",
        "            embed(\"some text for testing\"), \n",
        "            embed(\"some text for testing\")\n",
        "        )) < 1e-4, \"Embedding should match\"\n",
        "\n",
        "assert abs(cosine(\n",
        "            embed(\"Cats eat mice.\"), \n",
        "            embed(\"Terminator is an autonomous cyborg, typically humanoid, originally conceived as a virtually indestructible soldier, infiltrator, and assassin.\")\n",
        "        )) > 0.2, \"Embeddings should be far\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA_KPCk6Yejk"
      },
      "source": [
        "## 3. Read the data\n",
        "\n",
        "Now, let's read the facts dataset. Download it from the abovementioned url and read to the list of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY4ko4SeYejk"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
        "\n",
        "#TODO read facts into a list of facts. Each fact is a separate element of array\n",
        "facts = []\n",
        "# raw = requests.get(url).text\n",
        "# facts = raw.split('\\n')\n",
        "\n",
        "facts = requests.get(url).text.split('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdpCfqqoYejl",
        "outputId": "99284521-119b-49fc-924b-2257cbf34e9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. If you somehow found a way to extract all of the gold from the bubbling core of our lovely little planet, you would be able to cover all of the land in a layer of gold up to your knees.\n",
            "2. McDonalds calls frequent buyers of their food \"heavy users.\"\n",
            "3. The average person spends 6 months of their lifetime waiting on a red light to turn green.\n",
            "4. The largest recorded snowflake was in Keogh, MT during year 1887, and was 15 inches wide.\n",
            "5. You burn more calories sleeping than you do watching television.\n"
          ]
        }
      ],
      "source": [
        "print(*facts[:5], sep='\\n')\n",
        "\n",
        "assert len(facts) == 159\n",
        "assert ('our lovely little planet') in facts[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBZusZiyYejl"
      },
      "source": [
        "## 4. Transform sentences to vectors\n",
        "\n",
        "Transform the list of facts to `numpy.array` of vectors corresponding to each document (`sent_vecs`), inferring them from the model we just loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-6sK2RWYejl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#TODO infer vectors\n",
        "\n",
        "# sent_vecs = np.array((len(facts), 512))\n",
        "# for j, val in enumerate(sent_vecs):\n",
        "#   sent_vecs[j] = embed(facts[j])\n",
        "sent_vecs = np.array([embed(f) for f in facts])\n",
        "# np.array([embed(t) for t in facts])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jydX3NRYejm"
      },
      "outputs": [],
      "source": [
        "assert sent_vecs.shape[0] == len(facts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCcDv32MYejm"
      },
      "source": [
        "## 5. Find closest to the query\n",
        "\n",
        "Now find 5 facts which are the closest to the query using cosine measure.\n",
        "\n",
        "### 5.1. Closest search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdqbaKsYYejm"
      },
      "outputs": [],
      "source": [
        "def find_k_closest(query, dataset, k=10):\n",
        "    # TODO your code gere\n",
        "\n",
        "    return np.argsort(dataset @ query)[-k:]\n",
        "    # return range(k)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrUcryNyYejn"
      },
      "source": [
        "### 5.1. Use your function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OARW12w6Yejn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cceb388e-d00c-4bce-ac6b-73334acc0bbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results for query: good mood\n",
            "\n",
            "\t 44. Honey never spoils.\n",
            "\t 45. About half of all Americans are on a diet on any given day.\n",
            "\t 98. Blue-eyed people tend to have the highest tolerance of alcohol.\n",
            "\t 68. Cherophobia is the fear of fun.\n",
            "\t 57. Gorillas burp when they are happy\n"
          ]
        }
      ],
      "source": [
        "query = \"good mood\"\n",
        "query_vec = embed(query)\n",
        "\n",
        "print(\"Results for query:\", query)\n",
        "print()\n",
        "for k in find_k_closest(query_vec, sent_vecs, 5):\n",
        "    print(\"\\t\", facts[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dREJT62Yejo"
      },
      "source": [
        "## 6. Measure DCG@5 for the following query bucket\n",
        "```\n",
        "good mood\n",
        "gorilla\n",
        "woman\n",
        "earth\n",
        "japan\n",
        "people\n",
        "math\n",
        "```\n",
        "\n",
        "Recommend 5 facts to each of the queries. Write your code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E85UQerMYejo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ace4b18e-966d-4836-b2ed-d728e2a51c3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "depress\n",
            "\t 29. Chewing gum while you cut an onion will help keep you from crying.\n",
            "\t 136. You can't kill yourself by holding your breath.\n",
            "\t 69. The toothpaste \"Colgate\" in Spanish translates to \"go hang yourself\"\n",
            "\t 63. Apple launched a clothing line in 1986. It was described as a \"train wreck\" by others.\n",
            "\t 61. A man from Britain changed his name to Tim Pppppppppprice to make it harder for telemarketers to pronounce.\n",
            "dinosaur\n",
            "\t 106. The male ostrich can roar just like a lion.\n",
            "\t 35. A crocodile can't poke its tongue out.\n",
            "\t 90. You are born with 300 bones, but by the time you are an adult you only have 206.\n",
            "\t 113. The tongue of a blue whale weighs more than most elephants!\n",
            "\t 117. A hummingbird weighs less than a penny.\n",
            "man\n",
            "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
            "\t 151. Women have twice as many pain receptors on their body than men. But a much higher pain tolerance.\n",
            "\t 6. There are more lifeforms living on your skin than there are people on the planet.\n",
            "\t 61. A man from Britain changed his name to Tim Pppppppppprice to make it harder for telemarketers to pronounce.\n",
            "\t 154. The total weight of all those ants, however, is about the same as all the humans.\n",
            "pregnancy\n",
            "\t 131. If a pregnant woman has organ damage, the baby in her womb sends stem cells to help repair the organ.\n",
            "\t 151. Women have twice as many pain receptors on their body than men. But a much higher pain tolerance.\n",
            "\t 53. Only 8% of dieters will follow a restrictive weight loss plan (such as hCG Drops diet, garcinia cambogia diet, etc.)\n",
            "\t 50. If you try to suppress a sneeze, you can rupture a blood vessel in your head or neck and die.\n",
            "\t 135. On average, 12 newborns will be given to the wrong parents daily.\n",
            "hello\n",
            "\t 112. A tarantula can live without food for more than two years.\n",
            "\t 38. Drying fruit depletes it of 30-80% of its vitamin and antioxidant content\n",
            "\t 26. Bob Marley's last words to his son before he died were \"Money can't buy life.\"\n",
            "\t 117. A hummingbird weighs less than a penny.\n",
            "\t 116. Male dogs lift their legs when they are urinating for a reason. They are trying to leave their mark higher so that it gives off the message that they are tall and intimidating.\n",
            "cat\n",
            "\t 110. Cats have 32 muscles in each of their ears.\n",
            "\t 114. Ever wonder where the phrase \"It's raining cats and dogs\" comes from? In the 17th century many homeless cats and dogs would drown and float down the streets of England, making it look like it literally rained cats and dogs.\n",
            "\t 85. The elephant is the only mammal that can't jump!\n",
            "\t 89. The bloodhound is the only animal whose evidence is admissible in court.\n",
            "\t 20. A coyote can hear a mouse moving underneath a foot of snow.\n",
            "alcohol\n",
            "\t 98. Blue-eyed people tend to have the highest tolerance of alcohol.\n",
            "\t 81. Under the Code of Hammurabi, bartenders who watered down beer were punished by execution.\n",
            "\t 43. Ketchup was used as a medicine back in the 1930's.\n",
            "\t 17. Coca-Cola would be green if coloring wasn't added to it.\n",
            "\t 134. A person can live without food for about a month, but only about a week without water. If the amount of water in your body is reduced by just 1%, you'll feel thirsty. If it's reduced by 10%, you'll die.\n",
            "sun\n",
            "\t 38. Drying fruit depletes it of 30-80% of its vitamin and antioxidant content\n",
            "\t 152. There are more stars in space than there are grains of sand on every beach in the world.\n",
            "\t 88. Earth is the only planet that is not named after a god.\n",
            "\t 129. In the past 20 years, scientists have found over 1,000 planets outside of our solar system.\n",
            "\t 127. 68% of the universe is dark energy, and 27% is dark matter; both are invisible, even with our powerful telescopes. This means we have only seen 5% of the universe from earth.\n"
          ]
        }
      ],
      "source": [
        "bucket = \"\"\"depress\n",
        "dinosaur\n",
        "man\n",
        "pregnancy\n",
        "hello\n",
        "cat\n",
        "alcohol\n",
        "sun\"\"\".split('\\n')\n",
        "\n",
        "for term in bucket:\n",
        "    print(term)\n",
        "    for k in find_k_closest(embed(term), sent_vecs, k=5)[::-1]:\n",
        "        print(\"\\t\", facts[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yapm-hJjYejp"
      },
      "source": [
        "## 2.7. [BONUS] Write your own relevance assessments and compute DCG@5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5F3zU_6HYejp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a291e9-7244-48e5-e2d4-510409da9dfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DCG@5 = 1.0388\n",
            "IDCG@5 = 2.9485\n",
            "nDCG@5 = 0.3523\n"
          ]
        }
      ],
      "source": [
        "assessments = [\n",
        "    [0, 1, 1, 0, 0], # good mood\n",
        "    [0, 1, 0, 1, 0], # gorilla\n",
        "    [0, 1, 0, 1, 0], # man\n",
        "    [1, 0, 0, 0, 1], # [preg]\n",
        "    [0, 0, 0, 0, 0], # Japan\n",
        "    [1, 1, 0, 0, 0], # cat\n",
        "    [1, 0, 0, 0, 0] # alcohol\n",
        "]\n",
        "\n",
        "optimal = [[1] * 5] * 7\n",
        "\n",
        "def dcg(rels):\n",
        "    from math import log\n",
        "    s = 0\n",
        "    for i, rel in enumerate(rels):\n",
        "        s += rel / log(1 + i + 1, 2)\n",
        "    return s\n",
        "\n",
        "dcg5 = sum([dcg(row) for row in assessments]) / len(assessments)\n",
        "idcg5 = sum([dcg(row) for row in optimal]) / len(optimal)\n",
        "\n",
        "print(f\"DCG@5 = {dcg5:.4f}\")\n",
        "print(f\"IDCG@5 = {idcg5:.4f}\")\n",
        "print(f\"nDCG@5 = {dcg5 / idcg5:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}